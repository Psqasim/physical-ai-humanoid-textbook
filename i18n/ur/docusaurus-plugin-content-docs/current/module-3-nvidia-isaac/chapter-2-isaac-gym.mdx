---
sidebar_position: 3
title: Chapter 2 - Isaac Gym for RL
---

import ChapterActionsBar from '@site/src/components/learning/ChapterActionsBar';

# Chapter 2: Reinforcement Learning کے لیے Isaac Gym

<ChapterActionsBar chapterTitle="Isaac Gym for RL" />

## تعارف

ہیومنائیڈ روبوٹ کو چلنا سکھانا روبوٹکس میں سب سے مشکل مسائل میں سے ایک ہے۔ پہیے والے روبوٹس کے برعکس جو صرف گھومتے ہیں، ہیومنائیڈز کو دو ٹانگوں پر مسلسل توازن رکھنا چاہیے جبکہ درجنوں جوائنٹس—hips، knees، ankles، arms—کو ہم آہنگ کرتے ہوئے استحکام برقرار رکھنا ہوتا ہے۔ روایتی کنٹرول طریقوں کے لیے ہاتھ سے ٹیون کیے گئے PID کنٹرولرز اور inverse kinematics کی ضرورت ہوتی ہے، جو انجینئرز کو کامل کرنے میں مہینے لگتے ہیں۔ پھر بھی، یہ کنٹرولرز اکثر ناہموار زمین پر یا دھکا دیے جانے پر ناکام ہو جاتے ہیں۔

**Reinforcement Learning (RL)** ایک بالکل مختلف نقطہ نظر پیش کرتا ہے: چلنے کے رویے کو پروگرام کرنے کے بجائے، آپ روبوٹ کو آزمائش اور غلطی کے ذریعے اسے **سیکھنے** دیتے ہیں۔ آپ reward function کی وضاحت کرتے ہیں (مثال کے طور پر، "گرے بغیر آگے بڑھیں")، اور روبوٹ لاکھوں actions کو explore کرتا ہے، آہستہ آہستہ دریافت کرتے ہوئے کہ کون سے joint torques مستحکم چال پیدا کرتے ہیں۔ مسئلہ؟ ٹریننگ میں 10-100 ملین simulation steps کی ضرورت ہوتی ہے—CPU پر sequentially چلانا مہینے لگ سکتا ہے۔

**Isaac Gym** کا داخلہ ہوتا ہے: NVIDIA کا GPU-accelerated RL framework جو **ایک GPU پر 4,000-16,000 parallel simulations چلاتا ہے**۔ CPUs پر جو کام مہینے لیتا ہے وہ RTX 4090 یا A100 پر **گھنٹوں** میں مکمل ہو جاتا ہے۔ اس chapter میں، آپ صفر سے bipedal humanoid کو چلنا سکھائیں گے، اسے 2 گھنٹے سے کم ٹریننگ ٹائم میں فوری طور پر گرنے سے مستحکم locomotion تک ترقی کرتے ہوئے دیکھیں گے۔

## Isaac Gym کیوں؟ رفتار کا انقلاب

روایتی RL frameworks (Gym، PyBullet، MuJoCo) CPUs پر چلتے ہیں، ایک وقت میں ایک environment کو simulate کرتے ہوئے۔ multi-process parallelization بھی رکاوٹوں سے ٹکراتا ہے:
- **Communication overhead**: processes کے درمیان states/actions کو منتقل کرنا سست ہے
- **CPU-bound physics**: Physics updates GPU cores کا فائدہ نہیں اٹھا سکتے
- **محدود parallelism**: 1,000 processes چلانا زیادہ تر نظاموں کو crash کر دیتا ہے

**Isaac Gym کا GPU-Native ڈیزائن**:
- **GPU پر Physics**: تمام rigid body dynamics CUDA cores پر چلتی ہیں
- **Parallel environments**: GPU memory میں بیک وقت 1,000-16,000 روبوٹس کو simulate کریں
- **صفر ڈیٹا منتقلی**: States، actions، rewards مکمل طور پر GPU پر compute ہوتے ہیں—کوئی CPU roundtrips نہیں
- **Tensor interface**: States/actions تک PyTorch tensors کے طور پر براہ راست رسائی

**کارکردگی کا موازنہ** (bipedal walker کو 10M steps کے لیے تربیت دینا):

| طریقہ | ہارڈویئر | Wall Time | لاگت (cloud) |
|--------|----------|-----------|--------------|
| Single CPU env (Gym) | 16-core CPU | 120 hours | $480 |
| 100 parallel CPUs (MPI) | 100-core cluster | 12 hours | $600 |
| Isaac Gym (2048 envs) | RTX 4090 | 1.5 hours | $3 |
| Isaac Gym (4096 envs) | A100 GPU | 45 minutes | $2 |

**80x speedup، 200x لاگت میں کمی۔**

## Isaac Gym کی تنصیب

Isaac Gym، Isaac Sim سے علیحدہ تقسیم کیا جاتا ہے (وہ complementary ہیں لیکن الگ tools ہیں)۔

### Prerequisites

- **NVIDIA GPU**: GTX 1660 یا بہتر (RTX 30 series+ تجویز کردہ)
- **CUDA**: 11.3+ (`nvcc --version` سے چیک کریں)
- **Ubuntu**: 20.04 یا 22.04
- **Python**: 3.7-3.10
- **PyTorch**: 1.11+ CUDA support کے ساتھ

### Installation Steps

```bash
# 1. Download Isaac Gym from NVIDIA
# Visit: https://developer.nvidia.com/isaac-gym
# Download "Isaac Gym Preview 4" (latest as of 2024)
# Requires NVIDIA Developer account (free)

# 2. Extract the archive
cd ~/Downloads
tar -xf IsaacGym_Preview_4_Package.tar.gz
cd isaacgym

# 3. Create a Python virtual environment
python3 -m venv ~/.isaac-gym-venv
source ~/.isaac-gym-venv/bin/activate

# 4. Install Isaac Gym Python package
cd python
pip install -e .

# 5. Install PyTorch with CUDA support (if not already installed)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# 6. Verify installation
cd ../
python examples/1080_balls_of_solitude.py

# You should see 1080 balls bouncing in a window (GPU physics demo)
```

**Troubleshooting**:
- **ImportError: libpython3.8.so**: Python dev libraries انسٹال کریں: `sudo apt-get install python3.8-dev`
- **CUDA mismatch**: یقینی بنائیں کہ PyTorch CUDA version آپ کے سسٹم CUDA سے مماثل ہے (`nvcc --version`)
- **Segmentation fault on launch**: NVIDIA drivers کو 515+ میں اپ ڈیٹ کریں

## Isaac Gym کی Architecture کو سمجھنا

Isaac Gym روایتی RL environments (Gym، Brax، MuJoCo) سے کئی طریقوں سے مختلف ہے:

### روایتی RL Flow (Gym + PyBullet)
```
1. env.reset() → returns state (numpy array)
2. action = policy(state)
3. state, reward, done = env.step(action)
4. [Repeat 2-3 for N steps]
5. Copy transitions to GPU for NN training
```

**Bottleneck**: Steps 1-4 CPU پر چلتے ہیں؛ training کے لیے GPU پر data کاپی ہوتا ہے → سست

### Isaac Gym Flow (GPU-Native)
```
1. env.reset_all() → returns state tensor (torch.Tensor on GPU)
2. action = policy(state)  # policy runs on GPU
3. state, reward, done = env.step(actions)  # physics on GPU
4. [Repeat 2-3 for N steps - all on GPU]
5. No data transfer - everything in GPU memory
```

**کلیدی بصیرت**: States، actions، rewards **PyTorch tensors** ہیں جو کبھی GPU نہیں چھوڑتے۔

### Vectorized API

Isaac Gym ایک **vectorized API** expose کرتا ہے: ایک environment کو step کرنے کے بجائے، آپ **تمام environments کو بیک وقت** step کرتے ہیں۔

```python
# Traditional (sequential)
for env in envs:
    state, reward, done = env.step(action)

# Isaac Gym (vectorized)
states, rewards, dones = envs.step(actions)  # Step 4096 envs at once
# states: Tensor(4096, obs_dim)
# rewards: Tensor(4096,)
# dones: Tensor(4096,)
```

یہ اس بات سے میل کھاتا ہے کہ GPUs کیسے کام کرتے ہیں: ہزاروں cores میں SIMD (Single Instruction، Multiple Data) parallelism۔

## اپنا پہلا Environment بنانا: Cartpole

Humanoids کو تربیت دینے سے پہلے، آئیے **Cartpole** سے آسان شروعات کریں (چلتی cart پر pole کو balance کریں)۔

### Environment Structure

```python
# cartpole_env.py
from isaacgym import gymapi
from isaacgym import gymutil
import torch
import numpy as np

class CartpoleEnv:
    def __init__(self, num_envs=2048, device='cuda:0'):
        self.num_envs = num_envs
        self.device = device

        # Initialize Gym
        self.gym = gymapi.acquire_gym()

        # Create simulation
        sim_params = gymapi.SimParams()
        sim_params.dt = 1.0 / 60.0  # 60 Hz
        sim_params.substeps = 2
        sim_params.up_axis = gymapi.UP_AXIS_Z
        sim_params.gravity = gymapi.Vec3(0.0, 0.0, -9.81)

        # Use GPU pipeline
        sim_params.use_gpu_pipeline = True
        sim_params.physx.use_gpu = True

        self.sim = self.gym.create_sim(0, 0, gymapi.SIM_PHYSX, sim_params)

        # Load assets
        self._create_envs()

        # Prepare for rendering
        self.gym.prepare_sim(self.sim)

        # Observation and action dimensions
        self.num_obs = 4  # [cart_pos, cart_vel, pole_angle, pole_angular_vel]
        self.num_acts = 1  # force applied to cart

        # Buffers (GPU tensors)
        self.obs_buf = torch.zeros((self.num_envs, self.num_obs), device=self.device)
        self.reward_buf = torch.zeros(self.num_envs, device=self.device)
        self.reset_buf = torch.ones(self.num_envs, device=self.device, dtype=torch.long)

    def _create_envs(self):
        # Define asset (cart + pole)
        asset_root = "assets"
        asset_file = "cartpole.urdf"

        asset_options = gymapi.AssetOptions()
        asset_options.fix_base_link = False
        cartpole_asset = self.gym.load_asset(self.sim, asset_root, asset_file, asset_options)

        # Create environments
        spacing = 2.0
        env_lower = gymapi.Vec3(-spacing, -spacing, 0.0)
        env_upper = gymapi.Vec3(spacing, spacing, spacing)

        self.envs = []
        for i in range(self.num_envs):
            env = self.gym.create_env(self.sim, env_lower, env_upper, int(np.sqrt(self.num_envs)))
            cartpole_handle = self.gym.create_actor(env, cartpole_asset, gymapi.Transform(), "cartpole", i, 1, 0)
            self.envs.append(env)

    def reset(self):
        # Reset all environments
        self.obs_buf[:] = 0
        self.reward_buf[:] = 0
        self.reset_buf[:] = 0
        return self.obs_buf

    def step(self, actions):
        # Apply actions (forces to cart)
        forces = actions * 10.0  # Scale action
        for i, env in enumerate(self.envs):
            self.gym.apply_actor_dof_forces(env, 0, forces[i].cpu().numpy())

        # Step physics
        self.gym.simulate(self.sim)
        self.gym.fetch_results(self.sim, True)

        # Compute observations and rewards
        self._compute_observations()
        self._compute_rewards()

        return self.obs_buf, self.reward_buf, self.reset_buf

    def _compute_observations(self):
        # Read state from simulation (cart position, pole angle, etc.)
        # Simplified for clarity
        dof_states = self.gym.acquire_dof_state_tensor(self.sim)
        self.obs_buf[:, 0] = dof_states[:, 0]  # cart position
        self.obs_buf[:, 1] = dof_states[:, 1]  # cart velocity
        # ... (extract pole angle and angular velocity)

    def _compute_rewards(self):
        # Reward = time alive - penalty for tipping over
        pole_angle = self.obs_buf[:, 2]
        self.reward_buf = 1.0 - torch.abs(pole_angle) / np.pi

        # Reset if pole falls (|angle| > 45 degrees)
        self.reset_buf = torch.where(torch.abs(pole_angle) > 0.785, 1, 0)
```

### PPO کے ساتھ تربیت

```python
# train_cartpole.py
import torch
import torch.nn as nn
from torch.distributions import Normal

# Simple policy network
class Policy(nn.Module):
    def __init__(self, obs_dim, act_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(obs_dim, 64), nn.ReLU(),
            nn.Linear(64, 64), nn.ReLU(),
            nn.Linear(64, act_dim)
        )

    def forward(self, obs):
        return torch.tanh(self.net(obs))

# Initialize environment and policy
env = CartpoleEnv(num_envs=4096)
policy = Policy(env.num_obs, env.num_acts).cuda()
optimizer = torch.optim.Adam(policy.parameters(), lr=3e-4)

# Training loop
obs = env.reset()
for step in range(10000):
    # Collect rollouts
    actions = policy(obs)
    obs, rewards, dones = env.step(actions)

    # Compute policy gradient loss (simplified PPO)
    loss = -rewards.mean()  # Maximize reward
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if step % 100 == 0:
        print(f"Step {step}, Avg Reward: {rewards.mean().item():.2f}")
```

**نتیجہ**: Cartpole ~1,000 steps (RTX 4090 پر 30 سیکنڈ) میں balance کرنا سیکھ لیتا ہے۔

## Humanoid کو چلنا سکھانا

اب اصل چیلنج: **bipedal humanoid کو تربیت دینا** Ant یا Humanoid assets استعمال کرتے ہوئے۔

### Step 1: Humanoid Environment لوڈ کریں

Isaac Gym ایک پہلے سے بنا ہوا `Humanoid` environment شامل کرتا ہے:

```python
from isaacgymenvs.tasks.humanoid import Humanoid
from isaacgymenvs.utils.rlgames_utils import RLGPUEnv

# Create 4096 parallel humanoid environments
envs = RLGPUEnv(num_envs=4096, task="Humanoid")

# Observation space: 108-dimensional (joint angles, velocities, orientation)
# Action space: 21-dimensional (torques for 21 DoF)
```

### Step 2: Reward Function کی وضاحت کریں

Reward function یہ شکل دیتا ہے کہ روبوٹ کیا سیکھتا ہے۔ چلنے کے لیے:

```python
def compute_humanoid_reward(
    obs_buf,
    progress_buf,
    actions,
    up_weight=0.1,
    heading_weight=0.5,
    energy_weight=0.05
):
    # 1. Stay upright (penalize tilting)
    up_vector = obs_buf[:, 2]  # Z-component of torso orientation
    up_reward = torch.clamp(up_vector, 0, 1)

    # 2. Move forward (reward velocity in X direction)
    forward_vel = obs_buf[:, 10]
    heading_reward = forward_vel

    # 3. Minimize energy usage (penalize large torques)
    energy_penalty = torch.sum(actions ** 2, dim=-1)

    # Combined reward
    reward = (
        up_weight * up_reward +
        heading_weight * heading_reward -
        energy_weight * energy_penalty
    )

    return reward
```

**ڈیزائن کے انتخاب**:
- **Upright bonus**: روبوٹ کو رینگنے سے روکتا ہے (عام ناکامی کا موڈ)
- **Forward velocity**: حرکت کی حوصلہ افزائی کرتا ہے
- **Energy penalty**: موثر چال کو فروغ دیتا ہے (کم flailing)

### Step 3: RL Games (PPO Library) کے ساتھ تربیت دیں

Isaac Gym **RL Games** کے ساتھ integrate ہوتا ہے، ایک high-performance PPO implementation:

```bash
# Install RL Games
pip install rl-games

# Train humanoid
cd isaacgymenvs
python train.py task=Humanoid headless=True num_envs=4096
```

**ٹریننگ پیرامیٹرز** (`humanoid.yaml` سے):
```yaml
name: Humanoid

env:
  numEnvs: 4096
  episodeLength: 1000
  enableDebugVis: False

train:
  algorithm: PPO
  horizon_length: 16
  minibatch_size: 8192
  num_epochs: 5
  learning_rate: 3e-4
  gamma: 0.99  # Discount factor
  clip_value: 0.2
```

### Step 4: ٹریننگ پروگریس کی نگرانی کریں

```python
# Training metrics (logged to TensorBoard)
# - reward_mean: Average episode reward
# - episode_length_mean: How long before falling
# - fps: Simulation frames per second

# Typical learning curve:
# 0-1M steps: Robot falls immediately (reward ~0)
# 1M-5M steps: Learns to stand briefly (reward ~50)
# 5M-10M steps: Takes first wobbly steps (reward ~200)
# 10M-20M steps: Stable walking emerges (reward ~500+)
```

**TensorBoard لانچ کریں**:
```bash
tensorboard --logdir runs/
```

لائیو ٹریننگ گرافس دیکھنے کے لیے `localhost:6006` پر جائیں۔

### Step 5: Trained Policy کا جائزہ لیں

```bash
# Load checkpoint and run inference
python train.py task=Humanoid test=True checkpoint=runs/Humanoid_123456/nn/Humanoid.pth
```

**روبوٹ کو real-time میں چلتے ہوئے دیکھیں!** Camera follow mode کو toggle کرنے کے لیے viewer میں `V` دبائیں۔

## Sim-to-Real Transfer کے لیے Domain Randomization

کامل simulation میں تربیت یافتہ RL policies اکثر **reality gap** کی وجہ سے حقیقی روبوٹس پر ناکام ہو جاتی ہیں۔ **Domain randomization** ٹریننگ کے دوران policy کو variability سے روشناس کراتی ہے، اسے حقیقی دنیا کی غیر یقینی صورتحال کے لیے مضبوط بناتی ہے۔

### کیا Randomize کرنا ہے

```python
# In Isaac Gym environment setup
def apply_domain_randomization(self):
    # 1. Randomize physics parameters
    for env in self.envs:
        actor = self.gym.get_actor_rigid_body_count(env, 0)
        for i in range(actor):
            # Randomize mass (±20%)
            props = self.gym.get_actor_rigid_body_properties(env, 0)
            props[i].mass *= torch.rand(1).item() * 0.4 + 0.8

            # Randomize friction (0.5 - 1.5)
            self.gym.set_actor_rigid_body_properties(env, 0, props)

    # 2. Randomize actuator delays (5-15ms)
    self.actuator_delay = torch.rand(self.num_envs) * 0.01 + 0.005

    # 3. Add sensor noise
    self.obs_noise_scale = 0.05  # 5% Gaussian noise on observations

    # 4. Randomize ground friction per episode
    self.ground_friction = torch.rand(self.num_envs) * 0.5 + 0.7
```

### Reset پر Randomization لاگو کریں

```python
def reset(self):
    self.apply_domain_randomization()
    # ... rest of reset logic
```

**اثر**: Domain randomization کے ساتھ تربیت یافتہ policies حقیقی روبوٹس پر **70-90% کامیابی کی شرح** حاصل کرتی ہیں بمقابلہ randomization کے بغیر **10-30%**۔

## Hands-On Exercise: Quadruped (ANT) کو تربیت دیں

**مقصد**: Isaac Gym استعمال کرتے ہوئے 4-ٹانگوں والے Ant robot کو چلنا سکھائیں۔

### Step 1: Ant Training لانچ کریں

```bash
cd isaacgymenvs
python train.py task=Ant num_envs=8192 headless=False
```

**مشاہدہ**: grid میں 8,192 ants ظاہر ہوتے ہیں۔ انہیں سیکھتے ہوئے دیکھنے کے لیے `Play` دبائیں۔

### Step 2: Reward Function میں ترمیم کریں

`isaacgymenvs/tasks/ant.py` میں ترمیم کریں:

```python
# Original reward: forward velocity only
reward = forward_vel

# Modified reward: add stability bonus
def compute_ant_reward(self):
    forward_vel = self.obs_buf[:, 13]
    torso_height = self.obs_buf[:, 2]

    # Reward forward motion
    forward_reward = forward_vel

    # Bonus for maintaining height (prevent crawling)
    height_bonus = torch.where(torso_height > 0.3, 1.0, 0.0)

    reward = forward_reward + 0.5 * height_bonus
    return reward
```

### Step 3: نتائج کا موازنہ کریں

دو versions کو تربیت دیں:
1. **Baseline** (اصل reward): اکثر رینگنا سیکھتا ہے
2. **Modified** (height bonus کے ساتھ): مناسب walking gait سیکھتا ہے

**موازنہ کرنے کے لیے Metrics**:
- 10M steps کے بعد average reward
- Average episode length
- Qualitative gait (video موازنہ)

### Step 4: Deployment کے لیے Policy Export کریں

```python
# Export trained policy as TorchScript (for deployment to real robots)
import torch

policy = torch.jit.load("runs/Ant/nn/Ant.pth")
scripted_policy = torch.jit.script(policy)
scripted_policy.save("ant_policy.pt")

# Deploy to NVIDIA Jetson (edge device)
# Load policy on robot:
# policy = torch.jit.load("ant_policy.pt")
# action = policy(observation)
```

## اعلیٰ درجے کے موضوعات

### 1. Curriculum Learning

بتدریج مشکل tasks کو تربیت دیں:
```python
# Week 1: Flat ground
# Week 2: Small bumps (5cm height)
# Week 3: Stairs (10cm steps)
# Week 4: Uneven terrain

def update_curriculum(self, progress):
    if progress < 0.25:
        self.terrain_difficulty = 0  # Flat
    elif progress < 0.5:
        self.terrain_difficulty = 1  # Bumps
    elif progress < 0.75:
        self.terrain_difficulty = 2  # Stairs
    else:
        self.terrain_difficulty = 3  # Random
```

### 2. Asymmetric Actor-Critic

ٹریننگ کے دوران **privileged information** استعمال کریں (ground truth state) لیکن صرف sensor observations کے ساتھ deploy کریں:

```python
# Critic sees ground truth (used only for training)
critic_obs = torch.cat([
    robot_state,          # Joint angles, velocities
    terrain_heightmap,    # Perfect terrain knowledge (privileged)
    external_forces       # Simulated wind (privileged)
], dim=-1)

# Actor sees only sensor data (used for deployment)
actor_obs = torch.cat([
    robot_state,
    noisy_imu_data,      # Realistic IMU with noise
    noisy_joint_encoders # Encoder readings with drift
], dim=-1)
```

**فائدہ**: Critic بہتر value estimates سیکھتا ہے (تیز تر ٹریننگ)، جبکہ actor مضبوط sensor-based policy سیکھتا ہے۔

### 3. Multi-Task Learning

متعدد skills کے لیے ایک policy کو تربیت دیں:
```python
# Task IDs: 0=walk, 1=run, 2=jump, 3=crouch
task_encoding = torch.nn.functional.one_hot(task_id, num_classes=4)
policy_input = torch.cat([observation, task_encoding], dim=-1)
```

## اگلے قدم

آپ نے اب Isaac Gym کے ساتھ GPU-accelerated reinforcement learning میں مہارت حاصل کر لی ہے:
- بڑے پیمانے پر parallel simulation (4,000+ environments)
- Bipedal اور quadrupedal locomotion کو تربیت دینا
- مضبوط sim-to-real transfer کے لیے domain randomization
- Edge devices (Jetson) پر policies کو deploy کرنا

**Chapter 3: Perception کے لیے Isaac ROS** (جلد آ رہا ہے) میں، آپ تربیت یافتہ RL policies کو real-time vision systems کے ساتھ integrate کریں گے۔ آپ NVIDIA Jetson hardware پر object detection، semantic segmentation، اور visual SLAM چلائیں گے، مکمل autonomy stack مکمل کرتے ہوئے: perception → planning → control۔

**Challenge Exercise**:
1. **مختلف reward functions** کے ساتھ Humanoid کو تربیت دیں (مثال کے طور پر، پیچھے چلنا، side-step، دوڑنا)
2. **انتہائی domain randomization** لاگو کریں (±50% mass variation، ±80% friction)
3. ایسے terrains پر policy کی **robustness** کا جائزہ لیں جو اس نے ٹریننگ کے دوران کبھی نہیں دیکھے
4. **Sample efficiency** کا موازنہ کریں: زیادہ سے زیادہ کارکردگی کے 80% تک پہنچنے میں کتنے timesteps لگتے ہیں؟
   - Baseline PPO
   - PPO + domain randomization
   - PPO + curriculum learning
5. **حقیقی روبوٹ میں Export کریں**: policy کو physical platform پر deploy کریں (اگر دستیاب ہو) اور sim-to-real transfer کی کامیابی کی شرح کی پیمائش کریں
