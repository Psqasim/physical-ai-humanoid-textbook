---
sidebar_position: 2
title: Chapter 1 - Introduction to VLA
---

import ChapterActionsBar from '@site/src/components/learning/ChapterActionsBar';

# Chapter 1: Introduction to Vision-Language-Action Models

<ChapterActionsBar chapterTitle="Introduction to VLA" />

## Introduction

In 2023, Google DeepMind demonstrated a humanoid robot that could follow complex, free-form instructions like **"throw away the trash in the recycling bin"**. The robot looked around the room, identified a crumpled paper ball on a table (visual perception), understood that "throw away" meant grasping and disposing (language grounding), selected the recycling bin over the trash can based on material type (reasoning), and executed a throwing motion (action generation)—all from a single sentence.

This capability stems from **Vision-Language-Action (VLA) models**, neural networks that integrate computer vision, natural language processing, and robotic control into a unified system. Unlike traditional robots that require separate modules for each component (object detection, command parsing, motion planning), VLAs learn an **end-to-end mapping** from pixels and words to motor commands.

In this chapter, you'll understand the architecture behind VLAs, run a pre-trained model (OpenVLA) to control a simulated robot, and evaluate how well these models generalize to novel instructions and environments. By the end, you'll appreciate why VLAs represent the most promising path toward general-purpose humanoid robots.

## The Evolution to VLAs: From Pixels to Actions

### 1. Traditional Computer Vision (2010s)

**Approach**: Train task-specific CNNs (AlexNet, ResNet, YOLO)
```
Image → CNN → Class labels ("mug," "bottle," "book")
```

**Limitations**:
- No language understanding (can't respond to "bring me a drink")
- No action generation (outputs labels, not motor commands)
- Requires labeled datasets per task (ImageNet for objects, COCO for segmentation)

### 2. Vision-Language Models (2020–2021)

**Breakthrough**: CLIP (OpenAI, 2021) learns to align images and text
```
Image → Vision Encoder → Embedding
Text → Language Encoder → Embedding
Similarity(image_embedding, text_embedding) → Match score
```

**Impact**:
- Zero-shot classification: Given text "a photo of a cat," CLIP can identify cats without cat-specific training
- Enables text-based image search ("show me images of sunsets over mountains")

**Limitations**:
- Still no action generation—outputs similarity scores, not robot commands

### 3. Vision-Language-Action (2022–Present)

**Breakthrough**: RT-1 (Google, 2022) extends vision-language models with action prediction
```
Image + Text → VLA Model → Robot actions (joint angles, gripper state)
```

**Innovation**: Treat actions as **tokens** in the model's vocabulary, alongside words and image patches.

**Example**:
```
Input: Image of a table + "pick up the red block"
Output: [0.45, 0.12, 0.30, ...] (7-DoF arm joint positions)
```

## Understanding VLA Architecture: The Three Pillars

### Pillar 1: Vision Encoder (Seeing)

**Purpose**: Convert raw pixels into semantic representations.

**Architecture Options**:

1. **Convolutional Neural Networks (CNNs)**
   - Examples: ResNet, EfficientNet
   - Pros: Fast, works on edge devices
   - Cons: Less effective for long-range dependencies

2. **Vision Transformers (ViT)**
   - Divides image into patches (e.g., 16x16 pixels)
   - Processes patches with self-attention
   - Pros: State-of-the-art accuracy, captures spatial relationships
   - Cons: Computationally expensive

3. **CLIP Encoder**
   - Pre-trained on 400M image-text pairs
   - Understands visual concepts + their linguistic descriptions
   - Pros: Zero-shot generalization to novel objects
   - Cons: Requires internet-scale pre-training data

**Example: ViT Processing**

```python
import torch
from transformers import ViTModel

# Load pre-trained ViT
vit = ViTModel.from_pretrained('google/vit-base-patch16-224')

# Input: 224x224 RGB image
image = torch.randn(1, 3, 224, 224)

# Forward pass
outputs = vit(pixel_values=image)

# Visual features: [batch_size, num_patches + 1, hidden_dim]
# 197 tokens: 196 patches (14x14 grid) + 1 CLS token
visual_features = outputs.last_hidden_state  # Shape: [1, 197, 768]

print(f"Visual tokens: {visual_features.shape}")
# Output: Visual tokens: torch.Size([1, 197, 768])
```

**Key Insight**: Each of the 197 tokens represents a spatial region of the image. The CLS token (first token) summarizes the entire image.

### Pillar 2: Language Encoder (Understanding)

**Purpose**: Encode natural language instructions into semantic vectors.

**Architecture Options**:

1. **BERT-style Encoders**
   - Examples: BERT, RoBERTa
   - Bidirectional attention (sees full sentence)
   - Pros: Strong semantic understanding
   - Cons: Cannot generate text (encoder-only)

2. **T5 (Text-to-Text Transfer Transformer)**
   - Encoder-decoder architecture
   - Treats all NLP tasks as text generation
   - Pros: Flexible, pre-trained on diverse tasks
   - Cons: Larger than BERT

3. **GPT-style Decoders**
   - Examples: GPT-3, GPT-4, LLaMA
   - Autoregressive generation (predicts next token)
   - Pros: Can generate complex reasoning chains
   - Cons: Unidirectional (only sees previous tokens during generation)

**Example: T5 Processing**

```python
from transformers import T5Tokenizer, T5EncoderModel

# Load T5 encoder
tokenizer = T5Tokenizer.from_pretrained('t5-base')
encoder = T5EncoderModel.from_pretrained('t5-base')

# Input: Natural language instruction
instruction = "Pick up the red block and place it in the box"

# Tokenize
inputs = tokenizer(instruction, return_tensors='pt', padding=True)
# Token IDs: [7, 734, 95, 8, 1131, 2463, 11, 286, 34, 16, 8, 3](example)

# Encode
outputs = encoder(**inputs)

# Language features: [batch_size, sequence_length, hidden_dim]
language_features = outputs.last_hidden_state  # Shape: [1, 12, 768]

print(f"Language tokens: {language_features.shape}")
# Output: Language tokens: torch.Size([1, 12, 768])
```

**Key Insight**: Each token represents a word or subword. The model learns contextual relationships ("red" modifies "block," "place" is the action on "it").

### Pillar 3: Action Head (Acting)

**Purpose**: Fuse vision and language to predict robot actions.

**Architecture**:

```python
class ActionHead(nn.Module):
    def __init__(self, hidden_dim=768, action_dim=7):
        super().__init__()
        # Cross-modal fusion
        self.cross_attention = nn.MultiheadAttention(hidden_dim, num_heads=8)

        # Action prediction
        self.action_mlp = nn.Sequential(
            nn.Linear(hidden_dim, 256),
            nn.ReLU(),
            nn.Linear(256, action_dim)
        )

    def forward(self, visual_tokens, language_tokens):
        # Cross-attend: language queries visual information
        fused, _ = self.cross_attention(
            query=language_tokens,      # What to do
            key=visual_tokens,           # Where things are
            value=visual_tokens          # Visual features
        )

        # Aggregate language tokens (mean pooling)
        fused_mean = fused.mean(dim=1)  # [batch_size, hidden_dim]

        # Predict actions
        actions = self.action_mlp(fused_mean)  # [batch_size, action_dim]
        return actions
```

**Action Representations**:

1. **Joint Positions** (Absolute)
   - Direct control: `[shoulder: 45°, elbow: 90°, wrist: 30°, ...]`
   - Pros: Precise, easy to interpret
   - Cons: Robot-specific, doesn't generalize across morphologies

2. **Joint Velocities** (Differential)
   - Rate of change: `[Δshoulder: +5°/s, Δelbow: -2°/s, ...]`
   - Pros: Smoother motion, easier to learn
   - Cons: Accumulates error over time

3. **End-Effector Pose** (Task Space)
   - 6D pose: `[x, y, z, roll, pitch, yaw]` + gripper state
   - Pros: Generalizes across robot arms (invariant to kinematics)
   - Cons: Requires inverse kinematics solver

**Example: Complete VLA Forward Pass**

```python
class SimpleVLA(nn.Module):
    def __init__(self, action_dim=7):
        super().__init__()
        self.vision_encoder = ViTModel.from_pretrained('google/vit-base-patch16-224')
        self.language_encoder = T5EncoderModel.from_pretrained('t5-base')
        self.action_head = ActionHead(hidden_dim=768, action_dim=action_dim)

    def forward(self, image, instruction_ids):
        # 1. Encode vision
        visual_outputs = self.vision_encoder(pixel_values=image)
        visual_tokens = visual_outputs.last_hidden_state  # [B, 197, 768]

        # 2. Encode language
        lang_outputs = self.language_encoder(input_ids=instruction_ids)
        lang_tokens = lang_outputs.last_hidden_state  # [B, L, 768]

        # 3. Fuse and predict actions
        actions = self.action_head(visual_tokens, lang_tokens)  # [B, action_dim]

        return actions

# Usage
model = SimpleVLA(action_dim=7)
image = torch.randn(1, 3, 224, 224)  # Camera image
instruction = tokenizer("pick up the cup", return_tensors='pt').input_ids

actions = model(image, instruction)
print(f"Predicted actions: {actions}")
# Output: Predicted actions: tensor([[ 0.45,  0.12, -0.30, ...]])
```

## Running a Pre-Trained VLA: OpenVLA Demo

**OpenVLA** is an open-source VLA released by UC Berkeley, designed to run on consumer hardware.

### Step 1: Install Dependencies

```bash
# Create virtual environment
python3 -m venv ~/vla-env
source ~/vla-env/bin/activate

# Install OpenVLA
pip install openvla transformers torch torchvision

# Verify installation
python -c "import openvla; print('OpenVLA version:', openvla.__version__)"
```

### Step 2: Load Pre-Trained Model

```python
# load_openvla.py
from openvla import OpenVLA
import torch

# Load model (downloads ~2GB checkpoint)
model = OpenVLA.from_pretrained('openvla/openvla-7b')
model.eval()  # Set to evaluation mode

# Move to GPU if available
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = model.to(device)

print(f"Model loaded on {device}")
print(f"Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M")
```

### Step 3: Run Inference

```python
# inference.py
from PIL import Image
import numpy as np

# Load test image (robot camera view)
image = Image.open('robot_view.jpg').resize((224, 224))

# Instruction
instruction = "pick up the red block"

# Predict action
with torch.no_grad():
    action = model.predict(image=image, instruction=instruction)

print(f"Predicted action: {action}")
# Output: Predicted action: [0.52, 0.18, -0.25, 0.10, 0.05, -0.02, 1.0]
# [shoulder_pan, shoulder_lift, elbow, wrist1, wrist2, wrist3, gripper_open]
```

### Step 4: Visualize Attention Maps

```python
# Visualize which image regions the model attends to for each instruction token
import matplotlib.pyplot as plt

attention_map = model.get_attention_map(image, instruction)
# attention_map: [num_instruction_tokens, 14, 14] (14x14 image patches)

# Plot attention for "red" token
token_idx = 3  # "red" is 4th token
plt.imshow(attention_map[token_idx])
plt.title("Attention for 'red'")
plt.colorbar()
plt.show()

# The model should highlight red regions in the image
```

## Evaluating VLA Generalization

VLAs excel at **zero-shot generalization**—performing tasks they weren't explicitly trained on. Let's test this.

### Test 1: Novel Objects

```python
# Training: Model saw "pick up the mug"
# Test: "pick up the cup"
# Expected: Succeeds (cup ≈ mug in appearance and affordance)

action = model.predict(image=cup_image, instruction="pick up the cup")
# Verify: Action should grasp the cup
```

### Test 2: Synonym Robustness

```python
# Training: "place the object in the box"
# Test: "put the item inside the container"
# Expected: Succeeds (language encoder understands synonyms)

action = model.predict(image, "put the item inside the container")
```

### Test 3: Novel Compositions

```python
# Training: Seen "pick up red objects" and "pick up blocks" separately
# Test: "pick up the green sphere"
# Expected: Generalizes (composes "green" + "sphere")

action = model.predict(image, "pick up the green sphere")
```

### Test 4: Negation and Constraints

```python
# Test: "pick up the block but not the red one"
# Challenge: Requires understanding negation
# Expected: RT-2 handles this (leverages LLM reasoning)

action = model.predict(image, "pick up the block but not the red one")
```

## Hands-On Exercise: VLA-Controlled Robot in Isaac Sim

**Objective**: Deploy OpenVLA to control a simulated robot arm in NVIDIA Isaac Sim.

### Step 1: Set Up Isaac Sim Environment

```python
# isaac_vla_env.py
from isaacgym import gymapi
import torch

# Initialize Isaac Sim
gym = gymapi.acquire_gym()
sim = gym.create_sim(0, 0, gymapi.SIM_PHYSX, gymapi.SimParams())

# Load Franka Panda robot arm
asset_root = "assets"
asset_file = "franka_panda.urdf"
franka_asset = gym.load_asset(sim, asset_root, asset_file)

# Create environment
env = gym.create_env(sim, gymapi.Vec3(-1, -1, 0), gymapi.Vec3(1, 1, 2), 1)
franka_handle = gym.create_actor(env, franka_asset, gymapi.Transform(), "franka", 0, 0)

# Add camera
camera_props = gymapi.CameraProperties()
camera_handle = gym.create_camera_sensor(env, camera_props)
gym.set_camera_location(camera_handle, env, gymapi.Vec3(1, 0, 1), gymapi.Vec3(0, 0, 0.5))

print("Isaac Sim environment ready")
```

### Step 2: Capture Camera Image

```python
# Render and capture RGB image
gym.step_graphics(sim)
gym.render_all_camera_sensors(sim)

# Get camera image
camera_image = gym.get_camera_image(sim, env, camera_handle, gymapi.IMAGE_COLOR)
image_array = np.reshape(camera_image, (224, 224, 4))[:, :, :3]  # Remove alpha channel

# Convert to PIL Image
from PIL import Image
pil_image = Image.fromarray(image_array.astype(np.uint8))
```

### Step 3: VLA Inference Loop

```python
# Load VLA model
from openvla import OpenVLA
vla_model = OpenVLA.from_pretrained('openvla/openvla-7b').cuda()

# Control loop
for step in range(1000):
    # 1. Get instruction from user (or predefined)
    instruction = "pick up the red block"

    # 2. Capture current camera view
    gym.step_graphics(sim)
    gym.render_all_camera_sensors(sim)
    camera_image = gym.get_camera_image(sim, env, camera_handle, gymapi.IMAGE_COLOR)
    image = Image.fromarray(np.reshape(camera_image, (224, 224, 4))[:, :, :3])

    # 3. Predict action with VLA
    with torch.no_grad():
        action = vla_model.predict(image=image, instruction=instruction)

    # 4. Apply action to robot
    action_tensor = torch.tensor(action[:7], dtype=torch.float32)  # 7-DoF joint positions
    gym.set_actor_dof_position_targets(env, franka_handle, action_tensor.cpu().numpy())

    # 5. Step simulation
    gym.simulate(sim)
    gym.fetch_results(sim, True)

    if step % 100 == 0:
        print(f"Step {step}: Executing '{instruction}'")
```

### Step 4: Evaluate Performance

```python
# Test different instructions
test_instructions = [
    "pick up the red block",
    "place the block in the box",
    "move the green sphere to the left",
    "grasp the mug by the handle"
]

success_count = 0
for instruction in test_instructions:
    # Run VLA
    success = run_vla_episode(instruction, max_steps=500)
    if success:
        success_count += 1

print(f"Success rate: {success_count}/{len(test_instructions)} ({success_count/len(test_instructions)*100:.1f}%)")
```

## Understanding Failure Modes

VLAs aren't perfect. Common failure cases:

### 1. Ambiguous Instructions
```
Instruction: "Pick it up"
Problem: What is "it"? No object specified
Solution: Require explicit object references
```

### 2. Physically Impossible Tasks
```
Instruction: "Lift the table"
Problem: Object too heavy for robot's actuators
Solution: Add affordance reasoning (predict feasibility before attempting)
```

### 3. Distribution Shift
```
Training: Brightly lit lab with white table
Testing: Dimly lit home with wooden table
Problem: Visual features differ (shadows, textures)
Solution: Domain randomization during training
```

### 4. Long-Horizon Tasks
```
Instruction: "Clean the entire kitchen"
Problem: Requires 100+ sub-actions (open drawer, grab cloth, wipe counter, ...)
Solution: Hierarchical planning (break into sub-goals)
```

## Next Steps

You've now mastered VLA fundamentals:
- Understanding the vision-language-action architecture
- Running pre-trained models (OpenVLA)
- Deploying VLAs in simulation (Isaac Sim)
- Evaluating generalization and failure modes

In **Chapter 2: Building and Deploying Your Own VLA**, you'll collect training data, fine-tune a VLA on custom tasks, integrate with ROS 2 for real robot control, and apply techniques like chain-of-thought prompting for complex multi-step procedures.

**Challenge Exercise**:
1. Modify the Isaac Sim environment to include **multiple objects** (blocks, spheres, cylinders)
2. Test VLA generalization: "pick up all the red objects"
3. Measure **action accuracy**: Does the predicted gripper pose match the object's location?
4. Compare performance with and without **vision pre-training** (random init vs. CLIP init)
5. Implement a **safety check**: If VLA predicts an action that would cause collision, halt and request human intervention
