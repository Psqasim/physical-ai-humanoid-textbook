---
sidebar_position: 1
title: Vision-Language-Action Overview
---

# Vision-Language-Action (VLA)

This module explores Vision-Language-Action models, the cutting-edge approach to building robots that understand natural language commands and execute physical tasks. You'll learn how multimodal AI enables intuitive human-robot interaction.

## What You'll Learn

- What are VLA models and how they work
- Training and fine-tuning VLA models for robotics
- Integrating VLA models with ROS 2 and Isaac
- Building end-to-end systems from language to action

## Prerequisites

- Completion of Modules 1, 2, and 3
- Understanding of transformer models (basic)
- Familiarity with PyTorch or TensorFlow (helpful)
