---
id: 0003
title: Deterministic Source Citations for RAG Responses
stage: implement
date: 2025-12-16
surface: agent
model: claude-opus-4-5
feature: 004-language-rag
branch: 001-multilingual-support
user: ps_qasim
command: /sp.implement
labels: ["rag", "citations", "sources", "deterministic", "multilingual"]
links:
  spec: specs/001-multilingual-support/spec.md
  ticket: null
  adr: null
  pr: null
files:
  - backend/app/services/qdrant.py
  - backend/app/services/rag.py
tests:
  - Sources come only from retrieved chunk metadata
  - Sources are deterministic (same chunks = same sources)
  - Sources header displays in correct language
---

## Prompt

NEXT TASK: Add source citations to grounded RAG responses

GOAL:
- Display source references for every answer
- Sources must come ONLY from retrieved chunk metadata

REQUIREMENTS:
- Do NOT let the LLM invent sources
- Extract sources from chunks (module_id, heading, url_path)
- Append a "ğŸ“š Sources" section AFTER the answer
- Keep answer language unchanged
- No frontend changes
- No ingestion changes

OUTPUT:
- Updated rag.py logic
- Example response with sources
- Confirmation sources are deterministic

## Response

### Implementation Summary

Added deterministic source citations that are extracted directly from retrieved chunk metadata, ensuring sources are always accurate and never hallucinated by the LLM.

### Changes Made

#### 1. `backend/app/services/qdrant.py` - Extended SearchResult

Added `language` and `url_path` fields to `SearchResult` dataclass:

```python
@dataclass
class SearchResult:
    id: str
    score: float
    doc_path: str
    module_id: str
    heading: str
    chunk_index: int
    text: str
    language: str = "en"    # NEW
    url_path: str = ""      # NEW
```

Updated `search_similar()` to populate these fields from Qdrant payload.

#### 2. `backend/app/services/rag.py` - Added format_sources_section()

New function that extracts sources deterministically from chunk metadata:

```python
def format_sources_section(chunks: List[SearchResult], language: LanguageCode = "en") -> str:
    """
    Format a deterministic sources section from retrieved chunks.
    Sources are extracted ONLY from chunk metadata - the LLM does not generate these.
    """
    source_headers = {
        "en": "ğŸ“š Sources:",
        "ur": "ğŸ“š Ù…Ø§Ø®Ø°:",
        "ja": "ğŸ“š å‡ºå…¸:",
    }

    # Deduplicate by doc_path
    seen_docs = set()
    for chunk in chunks:
        if chunk.doc_path in seen_docs:
            continue
        seen_docs.add(chunk.doc_path)

        # Format: "Heading - doc_path (url_path)"
        source_line = f"{chunk.heading} - {chunk.doc_path}"
        if chunk.url_path:
            source_line = f"{source_line} ({chunk.url_path})"
        sources.append(source_line)

    # Return numbered list, max 5 sources
    return f"\n\n{header}\n" + "\n".join(numbered_sources[:5])
```

#### 3. Updated `answer_chat_request()`

Appends sources section AFTER the LLM answer:

```python
# Sources extracted from metadata, NOT generated by LLM
sources_section = format_sources_section(chunks, language=response_language)
answer_with_sources = answer + sources_section

response = ChatResponse(
    answer=answer_with_sources,  # Answer + deterministic sources
    citations=citations,
    ...
)
```

### Example Response with Sources

**English:**
```
ROS 2 is a flexible robotics framework that provides tools for building
robot applications. It supports real-time communication through topics
and services...

ğŸ“š Sources:
1. Introduction to ROS 2 - docs/module-1/chapter-1.md (/docs/module-1/chapter-1)
2. Communication Patterns - docs/module-1/chapter-2.md (/docs/module-1/chapter-2)
3. Services and Actions - docs/module-1/chapter-3.md (/docs/module-1/chapter-3)
```

**Japanese:**
```
ROS 2ã¯ã€ãƒ­ãƒœãƒƒãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®ãƒ„ãƒ¼ãƒ«ã‚’æä¾›ã™ã‚‹
æŸ”è»Ÿãªãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™...

ğŸ“š å‡ºå…¸:
1. Introduction to ROS 2 - docs/module-1/chapter-1.md
2. Communication Patterns - docs/module-1/chapter-2.md
```

**Urdu:**
```
ROS 2 Ø§ÛŒÚ© Ù„Ú†Ú©Ø¯Ø§Ø± Ø±ÙˆØ¨ÙˆÙ¹Ú©Ø³ ÙØ±ÛŒÙ… ÙˆØ±Ú© ÛÛ’ Ø¬Ùˆ Ø±ÙˆØ¨ÙˆÙ¹ Ø§ÛŒÙ¾Ù„ÛŒ Ú©ÛŒØ´Ù†Ø²
Ø¨Ù†Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ù¹ÙˆÙ„Ø² ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’...

ğŸ“š Ù…Ø§Ø®Ø°:
1. Introduction to ROS 2 - docs/module-1/chapter-1.md
2. Communication Patterns - docs/module-1/chapter-2.md
```

### Why Sources are Deterministic

| Aspect | Implementation |
|--------|---------------|
| Source of truth | Chunk metadata only (doc_path, heading, url_path) |
| LLM involvement | None - sources appended AFTER LLM response |
| Deduplication | By doc_path to avoid duplicate sources |
| Ordering | By relevance score (order chunks were retrieved) |
| Maximum | 5 sources per response |
| Reproducibility | Same chunks â†’ same sources (always) |

### Data Flow

```
RAG Retrieval
    â†“
chunks = [SearchResult(doc_path, heading, url_path, ...)]
    â†“
generate_answer() â†’ LLM response (no sources)
    â†“
format_sources_section(chunks) â†’ deterministic sources from metadata
    â†“
answer_with_sources = answer + sources_section
    â†“
ChatResponse(answer=answer_with_sources)
```

### Outcome

- Sources are 100% deterministic and traceable
- LLM cannot invent or hallucinate sources
- Multilingual source headers (ğŸ“š Sources / Ù…Ø§Ø®Ø° / å‡ºå…¸)
- No frontend or ingestion changes required
