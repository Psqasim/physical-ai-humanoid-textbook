---
id: 0005
title: Multilingual Voice Mode (STT â†’ RAG â†’ TTS)
stage: implement
date: 2025-12-16
surface: agent
model: claude-opus-4-5
feature: 004-language-rag
branch: 001-multilingual-support
user: ps_qasim
command: /sp.implement
labels: ["voice", "stt", "tts", "whisper", "multilingual", "openai"]
links:
  spec: specs/001-multilingual-support/spec.md
  ticket: null
  adr: null
  pr: null
files:
  - backend/app/services/voice.py
  - backend/app/api/voice.py
  - backend/app/models/schemas.py
  - backend/app/main.py
tests:
  - Audio is transcribed via Whisper with language detection
  - Detected language is passed to RAG pipeline
  - TTS generates audio in same language as response
  - Voice endpoint accepts WAV/MP3/WebM formats
---

## Prompt

NEXT TASK: Implement multilingual Voice Mode (STT â†’ RAG â†’ TTS)

GOAL:
- Add voice-based Q&A using existing RAG system
- Auto-detect spoken language from audio
- Respond in the detected language

REQUIREMENTS:
- BACKEND ONLY (no frontend changes)
- Add Speech-to-Text using OpenAI Whisper
- Auto-detect language from audio (en, ur, ja)
- Pass detected language to answer_chat_request()
- Convert final answer text to speech (TTS) in same language
- Use OpenAI TTS API

FLOW:
1. User uploads audio â†’ Whisper STT
2. Auto-detect language from audio
3. Pass question + detected language to RAG
4. Generate answer in detected language
5. Convert answer to speech via TTS
6. Return transcription + answer + audio

OUTPUT:
- New voice service (voice.py)
- Voice API endpoint
- Updated schemas for voice response
- Create PHR

## Response snapshot

### Implementation Summary

Added complete multilingual voice mode with:
1. **Speech-to-Text (STT)**: OpenAI Whisper with auto language detection
2. **Language Detection**: Whisper detects spoken language (en/ur/ja)
3. **RAG Integration**: Passes detected language to existing `answer_chat_request()`
4. **Text-to-Speech (TTS)**: OpenAI TTS with language-appropriate voice selection

### Files Created/Modified

#### 1. `backend/app/services/voice.py` (NEW)

Voice service with STT and TTS capabilities:

```python
# Transcribe audio with Whisper
async def transcribe_audio(
    audio_data: bytes,
    filename: str = "audio.wav",
    preferred_language: LanguageCode | None = None,
) -> TranscriptionResult:
    """
    Transcribe audio using OpenAI Whisper with auto language detection.
    Returns: TranscriptionResult(text, detected_language, confidence, duration)
    """

# Generate speech from text
async def generate_speech(
    text: str,
    language: LanguageCode = "en",
    voice: str | None = None,
) -> bytes:
    """
    Generate MP3 audio from text using OpenAI TTS.
    Selects appropriate voice based on language.
    """
```

Voice selection per language:
- English: `alloy` (neutral, clear)
- Urdu: `nova` (works well for RTL)
- Japanese: `shimmer` (good pronunciation)

#### 2. `backend/app/api/voice.py` (NEW)

API endpoints:

```python
@router.post("/voice/chat")
async def voice_chat_endpoint(
    audio: UploadFile,
    generate_audio: bool = True,
    preferred_language: str | None = None,
) -> VoiceChatResponse:
    """
    1. Validate audio (WAV/MP3/WebM, max 25MB)
    2. Transcribe with Whisper (auto language detection)
    3. Process through RAG pipeline
    4. Generate TTS audio (optional)
    5. Return combined response
    """

@router.get("/voice/audio/{audio_id}")
async def get_voice_audio(audio_id: str) -> Response:
    """Retrieve cached TTS audio by ID."""
```

#### 3. `backend/app/models/schemas.py` (MODIFIED)

Added voice-related schemas:

```python
class VoiceTranscription(BaseModel):
    text: str
    detectedLanguage: Literal["en", "ur", "ja", "unknown"]
    confidence: Literal["high", "low"]
    durationSeconds: float | None

class VoiceChatResponse(BaseModel):
    transcription: VoiceTranscription
    answer: str
    citations: list[Citation]
    mode: Literal["whole-book", "selection"]
    confidence: Literal["high", "medium", "low"] | None
    responseLanguage: Literal["en", "ur", "ja"] | None
    fallbackApplied: bool | None
    hasAudio: bool
    audioId: str | None
```

#### 4. `backend/app/main.py` (MODIFIED)

Registered voice router:

```python
from app.api import health, chat, language, voice

app.include_router(voice.router, prefix="/api", tags=["voice"])
```

### API Flow

```
POST /api/voice/chat (multipart/form-data)
    â†“
    audio file (WAV/MP3/WebM)
    â†“
    Whisper STT â†’ text + detected_language
    â†“
    answer_chat_request(question=text, preferredLanguage=detected_lang)
    â†“
    RAG retrieval â†’ LLM answer
    â†“
    OpenAI TTS â†’ audio_bytes
    â†“
    VoiceChatResponse {
        transcription: { text, detectedLanguage, confidence },
        answer: "...",
        hasAudio: true,
        audioId: "abc123"
    }

GET /api/voice/audio/{audio_id}
    â†“
    MP3 audio response
```

### Example Response

```json
{
  "transcription": {
    "text": "ROS 2ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
    "detectedLanguage": "ja",
    "confidence": "high",
    "durationSeconds": 2.1
  },
  "answer": "ROS 2ã¯ã€ãƒ­ãƒœãƒƒãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®...\n\nğŸ“š å‡ºå…¸:\n1. Introduction to ROS 2 - docs/module-1/chapter-1.md",
  "citations": [...],
  "mode": "whole-book",
  "confidence": "high",
  "responseLanguage": "ja",
  "fallbackApplied": false,
  "hasAudio": true,
  "audioId": "f3a2b1c4"
}
```

### Language Detection Flow

| Audio Language | Whisper Detection | RAG Filter | Response Language | TTS Voice |
|---------------|-------------------|------------|-------------------|-----------|
| English       | en (high)         | en         | en                | alloy     |
| Urdu          | ur (high)         | ur         | ur                | nova      |
| Japanese      | ja (high)         | ja         | ja                | shimmer   |
| Unknown       | unknown (low)     | en (fallback) | en             | alloy     |

## Outcome

- âœ… Impact: Complete voice-based Q&A pipeline with auto language detection
- ğŸ§ª Tests: Audio transcription, language detection, TTS generation, API endpoints
- ğŸ“ Files: 4 files (2 new, 2 modified)
- ğŸ” Next prompts: Frontend voice UI integration, audio streaming optimization
- ğŸ§  Reflection: Clean separation - voice service handles STT/TTS, reuses existing RAG pipeline

## Evaluation notes (flywheel)

- Failure modes observed: None - clean implementation following existing patterns
- Graders run and results (PASS/FAIL): Manual verification needed
- Prompt variant (if applicable): N/A
- Next experiment (smallest change to try): Add audio streaming for faster TTS delivery
